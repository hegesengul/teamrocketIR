{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T14:51:45.135543Z",
     "start_time": "2025-06-06T14:51:45.131395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "\n",
    "ENTRY_COUNT = 5000\n",
    "BATCH_SIZE = 300\n",
    "OUTPUTS_BATCH_SIZE = 10\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "torch.set_default_device(device)"
   ],
   "id": "a1d195518fe5e5e0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T12:48:44.184671Z",
     "start_time": "2025-06-06T12:48:12.301344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download corpus\n",
    "\n",
    "import pyterrier as pt\n",
    "import json\n",
    "\n",
    "def download_dataset():\n",
    "    if not pt.java.started():\n",
    "        pt.java.init()\n",
    "    dataset = pt.get_dataset('irds:codesearchnet')\n",
    "\n",
    "    return list(map(lambda x: x[\"code\"], list(filter(lambda x: x[\"language\"]=='python', dataset.get_corpus_iter()))[:ENTRY_COUNT]))\n",
    "\n",
    "corpus = download_dataset()\n",
    "\n",
    "with open(\"entries.json\", \"w\") as f:\n",
    "    json.dump(corpus, f)"
   ],
   "id": "b0c4049f69e8dd47",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "codesearchnet documents: 100%|██████████| 2070536/2070536 [00:31<00:00, 65195.42it/s] \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:57:56.711662Z",
     "start_time": "2025-06-06T11:57:48.104744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up the tokenizer\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', use_fast=True)\n",
    "\n",
    "def tokenizer_func(data):\n",
    "    return tokenizer(data[\"code\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Set up dataset and create tokens\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"code\": corpus})\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenizer_func, batched=True, num_proc=6)\n",
    "\n",
    "tokenized_dataset.to_json(\"tokens.json\")"
   ],
   "id": "e397ef61ce94f10c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/teamrocketIR/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "Map (num_proc=6):   0%|          | 0/5000 [00:00<?, ? examples/s]\u001B[A\n",
      "Map (num_proc=6):  17%|█▋        | 833/5000 [00:00<00:02, 1545.35 examples/s]\u001B[A\n",
      "Map (num_proc=6):  50%|█████     | 2500/5000 [00:00<00:00, 4459.42 examples/s]\u001B[A\n",
      "Map (num_proc=6): 100%|██████████| 5000/5000 [00:00<00:00, 5433.43 examples/s]\u001B[A\n",
      "\n",
      "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]\u001B[A\n",
      "Creating json from Arrow format: 100%|██████████| 5/5 [00:00<00:00, 22.62ba/s]\u001B[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22280735"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T14:23:57.496122Z",
     "start_time": "2025-06-06T14:23:33.028601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up model and load tokens\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaModel\n",
    "from datasets import Dataset\n",
    "\n",
    "model = RobertaModel.from_pretrained('roberta-base').half()\n",
    "model.eval()\n",
    "\n",
    "tokenized_dataset = Dataset.from_json(\"tokens.json\")\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ],
   "id": "d7167d7a854711b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/teamrocketIR/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T14:35:06.774258Z",
     "start_time": "2025-06-06T14:24:12.528045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run model\n",
    "import os\n",
    "\n",
    "data_loader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_idx=0\n",
    "    batch_idx=0\n",
    "    for batch in data_loader:\n",
    "        if os.path.isfile(f\"./output_{out_idx}.pt\"):\n",
    "            batch_idx+=OUTPUTS_BATCH_SIZE\n",
    "            out_idx+=1\n",
    "            continue\n",
    "        output = model(**batch, output_hidden_states=True)\n",
    "        print(\"Progress:{} {:.5f}\".format(batch_idx, batch_idx / len(data_loader)))\n",
    "        results.append(output.last_hidden_state.cpu())\n",
    "\n",
    "        if len(results) == OUTPUTS_BATCH_SIZE or batch_idx == len(data_loader) - 1:\n",
    "            stacked_results = torch.cat(results)\n",
    "            torch.save(stacked_results, f\"output_{out_idx}.pt\")\n",
    "\n",
    "            out_idx += 1\n",
    "\n",
    "            results = []\n",
    "        batch_idx+=1"
   ],
   "id": "bf358ab6145b7f2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0 0.00000\n",
      "Progress:1 0.05882\n",
      "Progress:2 0.11765\n",
      "Progress:3 0.17647\n",
      "Progress:4 0.23529\n",
      "Progress:5 0.29412\n",
      "Progress:6 0.35294\n",
      "Progress:7 0.41176\n",
      "Progress:8 0.47059\n",
      "Progress:9 0.52941\n",
      "Progress:10 0.58824\n",
      "Progress:11 0.64706\n",
      "Progress:12 0.70588\n",
      "Progress:13 0.76471\n",
      "Progress:14 0.82353\n",
      "Progress:15 0.88235\n",
      "Progress:16 0.94118\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
