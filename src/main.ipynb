{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Download corpus\n",
    "\n",
    "import pyterrier as pt\n",
    "\n",
    "def download_dataset():\n",
    "    if not pt.java.started():\n",
    "        pt.java.init()\n",
    "    dataset = pt.get_dataset('irds:codesearchnet')\n",
    "\n",
    "    return list(map(lambda x: x[\"code\"], dataset.get_corpus_iter()))\n",
    "\n",
    "corpus = download_dataset()"
   ],
   "id": "b0c4049f69e8dd47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set up the tokenizer\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', use_fast=True)\n",
    "\n",
    "def tokenizer_func(data):\n",
    "    return tokenizer(data[\"code\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Set up dataset and create tokens\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"code\": corpus})\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenizer_func, batched=True, num_proc=6)\n",
    "\n",
    "tokenized_dataset.to_json(\"tokens.json\")"
   ],
   "id": "e397ef61ce94f10c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set up model and load tokens\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaModel\n",
    "from datasets import Dataset\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.eval()\n",
    "\n",
    "tokenized_dataset = Dataset.from_json(\"tokens.json\")\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ],
   "id": "d7167d7a854711b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T15:32:00.720068Z",
     "start_time": "2025-04-27T15:30:03.729683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run model\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 300\n",
    "\n",
    "data_loader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    i=0\n",
    "    for batch in data_loader:\n",
    "        output = model(**batch, output_hidden_states=True)\n",
    "        print(\"Progress:{} {:.5f}\".format(i, i/len(data_loader)))\n",
    "        #results.append(output.last_hidden_state.cpu())\n",
    "        results.append(output.last_hidden_state.cpu().numpy())\n",
    "\n",
    "        if len(results) == 10:\n",
    "            #torch.save(torch.stack(results), f\"outputs/output{i-9}-{i}.pt\")\n",
    "            np.savez_compressed(f\"outputs/output{i-9}-{i}.npz\", np.stack(results))\n",
    "            results = []\n",
    "        i+=1"
   ],
   "id": "bf358ab6145b7f2a",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m data_loader:\n\u001B[32m     13\u001B[39m     output = model(**batch, output_hidden_states=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mProgress:\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{:.5f}\u001B[39;00m\u001B[33m\"\u001B[39m.format(i, i/\u001B[38;5;28mlen\u001B[39m(data_loader)))\n\u001B[32m     15\u001B[39m     \u001B[38;5;66;03m#results.append(output.last_hidden_state.cpu())\u001B[39;00m\n\u001B[32m     16\u001B[39m     results.append(output.last_hidden_state.cpu().numpy())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/snap/pycharm-professional/475/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_frame.py:888\u001B[39m, in \u001B[36mPyDBFrame.trace_dispatch\u001B[39m\u001B[34m(self, frame, event, arg)\u001B[39m\n\u001B[32m    885\u001B[39m             stop = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    887\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m plugin_stop:\n\u001B[32m--> \u001B[39m\u001B[32m888\u001B[39m     stopped_on_plugin = plugin_manager.stop(main_debugger, frame, event, \u001B[38;5;28mself\u001B[39m._args, stop_info, arg, step_cmd)\n\u001B[32m    889\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m stop:\n\u001B[32m    890\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_line:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/snap/pycharm-professional/475/plugins/python-ce/helpers/jupyter_debug/pydev_jupyter_plugin.py:171\u001B[39m, in \u001B[36mstop\u001B[39m\u001B[34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[39m\n\u001B[32m    169\u001B[39m     frame = suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[32m    170\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m         main_debugger.do_wait_suspend(thread, frame, event, arg)\n\u001B[32m    172\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    173\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/snap/pycharm-professional/475/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[39m, in \u001B[36mPyDB.do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[39m\n\u001B[32m   1217\u001B[39m         from_this_thread.append(frame_id)\n\u001B[32m   1219\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, stop_reason):\n\u001B[32m-> \u001B[39m\u001B[32m1220\u001B[39m     \u001B[38;5;28mself\u001B[39m._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/snap/pycharm-professional/475/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[39m, in \u001B[36mPyDB._do_wait_suspend\u001B[39m\u001B[34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[39m\n\u001B[32m   1232\u001B[39m             \u001B[38;5;28mself\u001B[39m._call_mpl_hook()\n\u001B[32m   1234\u001B[39m         \u001B[38;5;28mself\u001B[39m.process_internal_commands()\n\u001B[32m-> \u001B[39m\u001B[32m1235\u001B[39m         time.sleep(\u001B[32m0.01\u001B[39m)\n\u001B[32m   1237\u001B[39m \u001B[38;5;28mself\u001B[39m.cancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[32m   1239\u001B[39m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
